{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from time import time\n",
    "\n",
    "from data_util import get_data_set, maybe_download_and_extract\n",
    "from model_util import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y, train_l = get_data_set(cifar=10)\n",
    "test_x, test_y, test_l = get_data_set(\"test\", cifar=10)\n",
    "\n",
    "x, y, output, global_step, y_pred_cls = model()\n",
    "\n",
    "_IMG_SIZE = 32\n",
    "_NUM_CHANNELS = 3\n",
    "_BATCH_SIZE = 256\n",
    "_CLASS_SIZE = 10\n",
    "_ITERATION = 5000\n",
    "_SAVE_PATH = \"/tmp/tensorboard/cifar-10/\"\n",
    "\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=y))\n",
    "\n",
    "\n",
    "starter_learning_rate = 1e-5\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                           500, 0.9, staircase=True)\n",
    "\n",
    "# Passing global_step to minimize() will increment it at each step.\n",
    "optimizer = (\n",
    "    tf.train.RMSPropOptimizer(learning_rate)\n",
    "    .minimize(loss, global_step=global_step)\n",
    ")\n",
    "#optimizer = tf.train.RMSPropOptimizer(learning_rate=1e-5).minimize(loss, global_step=global_step)\n",
    "\n",
    "\n",
    "correct_prediction = tf.equal(y_pred_cls, tf.argmax(y, dimension=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "tf.summary.scalar(\"Loss/train\", loss)\n",
    "tf.summary.scalar(\"Accuracy/train\", accuracy)\n",
    "\n",
    "\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "train_writer = tf.summary.FileWriter(_SAVE_PATH, sess.graph)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3072)\n",
      "(50000, 10)\n",
      "['airplane', 'automobile', 'bird', 'cat', 'deer']\n",
      "(50000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(train_l[:5])\n",
    "img_train_x = train_x.reshape(-1,32,32,3)\n",
    "print(img_train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(num_iterations):\n",
    "    '''\n",
    "        Train CNN\n",
    "    '''\n",
    "    for i in range(num_iterations):\n",
    "        randidx = np.random.randint(len(train_x), size=_BATCH_SIZE)\n",
    "        batch_xs = train_x[randidx]\n",
    "        batch_ys = train_y[randidx]\n",
    "\n",
    "        start_time = time()\n",
    "        i_global, _ = sess.run([global_step, optimizer], feed_dict={x: batch_xs, y: batch_ys})\n",
    "        duration = time() - start_time\n",
    "\n",
    "        if (i_global % 10 == 0) or (i == num_iterations - 1):\n",
    "            _loss, batch_acc = sess.run([loss, accuracy], feed_dict={x: batch_xs, y: batch_ys})\n",
    "            summary = tf.Summary(value=[\n",
    "                tf.Summary.Value(tag=\"Loss/test\", simple_value=_loss),\n",
    "            ])\n",
    "            train_writer.add_summary(summary, i_global)\n",
    "            msg = \"Global Step: {0:>6}, accuracy: {1:>6.1%}, loss = {2:.2f} ({3:.1f} examples/sec, {4:.2f} sec/batch)\"\n",
    "            print(msg.format(i_global, batch_acc, _loss, _BATCH_SIZE / duration, duration))\n",
    "\n",
    "        if (i_global % 100 == 0) or (i == num_iterations - 1):\n",
    "            \n",
    "            \n",
    "            data_merged, global_1 = sess.run([merged, global_step], feed_dict={x: batch_xs, y: batch_ys})\n",
    "            acc = predict_test()\n",
    "\n",
    "            summary = tf.Summary(value=[\n",
    "                tf.Summary.Value(tag=\"Accuracy/test\", simple_value=acc),\n",
    "            ])\n",
    "            train_writer.add_summary(data_merged, global_1)\n",
    "            train_writer.add_summary(summary, global_1)\n",
    "\n",
    "            saver.save(sess, save_path=_SAVE_PATH, global_step=global_step)\n",
    "            print(\"Saved checkpoint.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_test(show_confusion_matrix=False):\n",
    "    '''\n",
    "        Make prediction for all images in test_x\n",
    "    '''\n",
    "    i = 0\n",
    "    predicted_class = np.zeros(shape=len(test_x), dtype=np.int)\n",
    "    while i < len(test_x):\n",
    "        j = min(i + _BATCH_SIZE, len(test_x))\n",
    "        batch_xs = test_x[i:j, :]\n",
    "        batch_ys = test_y[i:j, :]\n",
    "        predicted_class[i:j] = sess.run(y_pred_cls, feed_dict={x: batch_xs, y: batch_ys})\n",
    "        i = j\n",
    "\n",
    "    correct = (np.argmax(test_y, axis=1) == predicted_class)\n",
    "    acc = correct.mean()\n",
    "    correct_numbers = correct.sum()\n",
    "    print(\"Accuracy on Test-Set: {0:.2f}% ({1} / {2})\".format(acc*100, correct_numbers, len(test_x)))\n",
    "\n",
    "    if show_confusion_matrix is True:\n",
    "        cm = confusion_matrix(y_true=np.argmax(test_y, axis=1), y_pred=predicted_class)\n",
    "        for i in range(_CLASS_SIZE):\n",
    "            class_name = \"({}) {}\".format(i, test_l[i])\n",
    "            print(cm[i, :], class_name)\n",
    "        class_numbers = [\" ({0})\".format(i) for i in range(_CLASS_SIZE)]\n",
    "        print(\"\".join(class_numbers))\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to restore last checkpoint ...\n",
      "INFO:tensorflow:Restoring parameters from None\n",
      "Failed to restore checkpoint. Initializing variables instead.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(\"Trying to restore last checkpoint ...\")\n",
    "    last_chk_path = tf.train.latest_checkpoint(checkpoint_dir=_SAVE_PATH)\n",
    "    saver.restore(sess, save_path=last_chk_path)\n",
    "    print(\"Restored checkpoint from:\", last_chk_path)\n",
    "except:\n",
    "    print(\"Failed to restore checkpoint. Initializing variables instead.\")\n",
    "    sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step:     10, accuracy:   7.0%, loss = 2.30 (942.8 examples/sec, 0.27 sec/batch)\n",
      "Global Step:     20, accuracy:  10.9%, loss = 2.30 (942.2 examples/sec, 0.27 sec/batch)\n",
      "Global Step:     30, accuracy:  10.5%, loss = 2.30 (911.0 examples/sec, 0.28 sec/batch)\n",
      "Global Step:     40, accuracy:   8.2%, loss = 2.30 (883.2 examples/sec, 0.29 sec/batch)\n",
      "Global Step:     50, accuracy:  10.2%, loss = 2.30 (952.5 examples/sec, 0.27 sec/batch)\n",
      "Global Step:     60, accuracy:   7.4%, loss = 2.31 (938.0 examples/sec, 0.27 sec/batch)\n",
      "Global Step:     70, accuracy:   9.8%, loss = 2.30 (919.5 examples/sec, 0.28 sec/batch)\n",
      "Global Step:     80, accuracy:  10.5%, loss = 2.30 (937.0 examples/sec, 0.27 sec/batch)\n",
      "Global Step:     90, accuracy:  11.7%, loss = 2.30 (959.4 examples/sec, 0.27 sec/batch)\n",
      "Global Step:    100, accuracy:   9.4%, loss = 2.30 (1024.4 examples/sec, 0.25 sec/batch)\n",
      "Accuracy on Test-Set: 9.99% (999 / 10000)\n",
      "Saved checkpoint.\n",
      "Global Step:    110, accuracy:  10.2%, loss = 2.30 (914.1 examples/sec, 0.28 sec/batch)\n",
      "Global Step:    120, accuracy:  10.9%, loss = 2.30 (944.9 examples/sec, 0.27 sec/batch)\n",
      "Global Step:    130, accuracy:   8.6%, loss = 2.30 (938.1 examples/sec, 0.27 sec/batch)\n",
      "Global Step:    140, accuracy:  12.5%, loss = 2.30 (955.2 examples/sec, 0.27 sec/batch)\n",
      "Global Step:    150, accuracy:   9.0%, loss = 2.30 (1036.4 examples/sec, 0.25 sec/batch)\n",
      "Global Step:    160, accuracy:   7.4%, loss = 2.30 (900.4 examples/sec, 0.28 sec/batch)\n",
      "Global Step:    170, accuracy:   8.6%, loss = 2.30 (954.0 examples/sec, 0.27 sec/batch)\n",
      "Global Step:    180, accuracy:  15.6%, loss = 2.30 (930.0 examples/sec, 0.28 sec/batch)\n",
      "Global Step:    190, accuracy:  11.7%, loss = 2.30 (881.6 examples/sec, 0.29 sec/batch)\n",
      "Global Step:    200, accuracy:  16.4%, loss = 2.30 (941.4 examples/sec, 0.27 sec/batch)\n",
      "Accuracy on Test-Set: 14.77% (1477 / 10000)\n",
      "Saved checkpoint.\n",
      "Global Step:    210, accuracy:  13.3%, loss = 2.30 (1033.1 examples/sec, 0.25 sec/batch)\n",
      "Global Step:    220, accuracy:  16.4%, loss = 2.30 (901.9 examples/sec, 0.28 sec/batch)\n",
      "Global Step:    230, accuracy:  16.4%, loss = 2.30 (978.1 examples/sec, 0.26 sec/batch)\n",
      "Global Step:    240, accuracy:  16.0%, loss = 2.29 (926.9 examples/sec, 0.28 sec/batch)\n",
      "Global Step:    250, accuracy:  18.4%, loss = 2.29 (943.7 examples/sec, 0.27 sec/batch)\n",
      "Global Step:    260, accuracy:  15.2%, loss = 2.29 (964.6 examples/sec, 0.27 sec/batch)\n",
      "Global Step:    270, accuracy:  13.7%, loss = 2.30 (876.5 examples/sec, 0.29 sec/batch)\n",
      "Global Step:    280, accuracy:  19.9%, loss = 2.29 (884.0 examples/sec, 0.29 sec/batch)\n",
      "Global Step:    290, accuracy:  16.0%, loss = 2.29 (892.3 examples/sec, 0.29 sec/batch)\n",
      "Global Step:    300, accuracy:  20.7%, loss = 2.29 (911.7 examples/sec, 0.28 sec/batch)\n",
      "Accuracy on Test-Set: 20.21% (2021 / 10000)\n",
      "Saved checkpoint.\n",
      "Global Step:    310, accuracy:  20.7%, loss = 2.29 (883.6 examples/sec, 0.29 sec/batch)\n",
      "Global Step:    320, accuracy:  18.0%, loss = 2.29 (941.3 examples/sec, 0.27 sec/batch)\n",
      "Global Step:    330, accuracy:  21.5%, loss = 2.28 (938.8 examples/sec, 0.27 sec/batch)\n",
      "Global Step:    340, accuracy:  14.5%, loss = 2.28 (930.2 examples/sec, 0.28 sec/batch)\n",
      "Global Step:    350, accuracy:  24.6%, loss = 2.27 (936.3 examples/sec, 0.27 sec/batch)\n",
      "Global Step:    360, accuracy:  21.1%, loss = 2.26 (937.1 examples/sec, 0.27 sec/batch)\n",
      "Global Step:    370, accuracy:  21.9%, loss = 2.27 (1026.1 examples/sec, 0.25 sec/batch)\n",
      "Global Step:    380, accuracy:  20.7%, loss = 2.26 (903.7 examples/sec, 0.28 sec/batch)\n",
      "Global Step:    390, accuracy:  20.7%, loss = 2.25 (941.4 examples/sec, 0.27 sec/batch)\n",
      "Global Step:    400, accuracy:  20.3%, loss = 2.25 (943.5 examples/sec, 0.27 sec/batch)\n",
      "Accuracy on Test-Set: 21.94% (2194 / 10000)\n",
      "Saved checkpoint.\n",
      "Global Step:    410, accuracy:  21.1%, loss = 2.24 (914.1 examples/sec, 0.28 sec/batch)\n",
      "Global Step:    420, accuracy:  17.6%, loss = 2.24 (889.9 examples/sec, 0.29 sec/batch)\n",
      "Global Step:    430, accuracy:  23.4%, loss = 2.22 (894.4 examples/sec, 0.29 sec/batch)\n",
      "Global Step:    440, accuracy:  21.1%, loss = 2.21 (946.8 examples/sec, 0.27 sec/batch)\n",
      "Global Step:    450, accuracy:  21.9%, loss = 2.21 (1037.6 examples/sec, 0.25 sec/batch)\n",
      "Global Step:    460, accuracy:  21.1%, loss = 2.20 (990.6 examples/sec, 0.26 sec/batch)\n",
      "Global Step:    470, accuracy:  24.2%, loss = 2.18 (1037.4 examples/sec, 0.25 sec/batch)\n",
      "Global Step:    480, accuracy:  19.9%, loss = 2.19 (885.9 examples/sec, 0.29 sec/batch)\n",
      "Global Step:    490, accuracy:  19.1%, loss = 2.20 (939.2 examples/sec, 0.27 sec/batch)\n",
      "Global Step:    500, accuracy:  19.9%, loss = 2.14 (868.1 examples/sec, 0.29 sec/batch)\n",
      "Accuracy on Test-Set: 19.62% (1962 / 10000)\n",
      "Saved checkpoint.\n",
      "Global Step:    510, accuracy:  25.0%, loss = 2.14 (888.9 examples/sec, 0.29 sec/batch)\n",
      "Global Step:    520, accuracy:  24.2%, loss = 2.15 (865.2 examples/sec, 0.30 sec/batch)\n",
      "Global Step:    530, accuracy:  24.6%, loss = 2.10 (976.3 examples/sec, 0.26 sec/batch)\n",
      "Global Step:    540, accuracy:  25.4%, loss = 2.11 (911.7 examples/sec, 0.28 sec/batch)\n",
      "Global Step:    550, accuracy:  31.6%, loss = 2.10 (914.3 examples/sec, 0.28 sec/batch)\n",
      "Global Step:    560, accuracy:  20.3%, loss = 2.12 (1012.4 examples/sec, 0.25 sec/batch)\n",
      "Global Step:    570, accuracy:  25.8%, loss = 2.09 (886.1 examples/sec, 0.29 sec/batch)\n",
      "Global Step:    580, accuracy:  28.9%, loss = 2.06 (1018.9 examples/sec, 0.25 sec/batch)\n",
      "Global Step:    590, accuracy:  23.0%, loss = 2.12 (1041.6 examples/sec, 0.25 sec/batch)\n",
      "Global Step:    600, accuracy:  24.2%, loss = 2.09 (926.6 examples/sec, 0.28 sec/batch)\n",
      "Accuracy on Test-Set: 24.76% (2476 / 10000)\n",
      "Saved checkpoint.\n",
      "Global Step:    610, accuracy:  22.7%, loss = 2.10 (937.4 examples/sec, 0.27 sec/batch)\n",
      "Global Step:    620, accuracy:  21.1%, loss = 2.10 (956.1 examples/sec, 0.27 sec/batch)\n",
      "Global Step:    630, accuracy:  28.1%, loss = 2.03 (864.9 examples/sec, 0.30 sec/batch)\n",
      "Global Step:    640, accuracy:  31.2%, loss = 2.01 (953.1 examples/sec, 0.27 sec/batch)\n",
      "Global Step:    650, accuracy:  26.6%, loss = 1.99 (955.8 examples/sec, 0.27 sec/batch)\n",
      "Global Step:    660, accuracy:  25.4%, loss = 2.06 (1038.1 examples/sec, 0.25 sec/batch)\n",
      "Global Step:    670, accuracy:  25.8%, loss = 2.02 (866.9 examples/sec, 0.30 sec/batch)\n",
      "Global Step:    680, accuracy:  27.3%, loss = 2.02 (900.0 examples/sec, 0.28 sec/batch)\n",
      "Global Step:    690, accuracy:  23.0%, loss = 2.00 (1038.2 examples/sec, 0.25 sec/batch)\n",
      "Global Step:    700, accuracy:  23.8%, loss = 2.05 (878.9 examples/sec, 0.29 sec/batch)\n",
      "Accuracy on Test-Set: 25.37% (2537 / 10000)\n",
      "Saved checkpoint.\n",
      "Global Step:    710, accuracy:  23.0%, loss = 2.03 (900.9 examples/sec, 0.28 sec/batch)\n",
      "Global Step:    720, accuracy:  25.8%, loss = 1.98 (934.2 examples/sec, 0.27 sec/batch)\n",
      "Global Step:    730, accuracy:  28.5%, loss = 2.01 (959.2 examples/sec, 0.27 sec/batch)\n",
      "Global Step:    740, accuracy:  25.4%, loss = 2.02 (1035.9 examples/sec, 0.25 sec/batch)\n",
      "Global Step:    750, accuracy:  23.8%, loss = 2.01 (884.7 examples/sec, 0.29 sec/batch)\n",
      "Global Step:    760, accuracy:  24.2%, loss = 2.02 (954.3 examples/sec, 0.27 sec/batch)\n",
      "Global Step:    770, accuracy:  19.5%, loss = 2.02 (941.2 examples/sec, 0.27 sec/batch)\n",
      "Global Step:    780, accuracy:  25.4%, loss = 1.98 (914.5 examples/sec, 0.28 sec/batch)\n",
      "Global Step:    790, accuracy:  25.0%, loss = 2.04 (974.4 examples/sec, 0.26 sec/batch)\n",
      "Global Step:    800, accuracy:  24.2%, loss = 2.02 (913.0 examples/sec, 0.28 sec/batch)\n",
      "Accuracy on Test-Set: 25.73% (2573 / 10000)\n",
      "Saved checkpoint.\n",
      "Global Step:    810, accuracy:  30.5%, loss = 2.00 (912.9 examples/sec, 0.28 sec/batch)\n",
      "Global Step:    820, accuracy:  26.6%, loss = 1.94 (969.1 examples/sec, 0.26 sec/batch)\n",
      "Global Step:    830, accuracy:  30.9%, loss = 1.98 (882.6 examples/sec, 0.29 sec/batch)\n",
      "Global Step:    840, accuracy:  28.1%, loss = 1.99 (939.5 examples/sec, 0.27 sec/batch)\n",
      "Global Step:    850, accuracy:  19.5%, loss = 2.01 (972.0 examples/sec, 0.26 sec/batch)\n",
      "Global Step:    860, accuracy:  30.1%, loss = 1.89 (934.5 examples/sec, 0.27 sec/batch)\n",
      "Global Step:    870, accuracy:  27.3%, loss = 2.02 (943.0 examples/sec, 0.27 sec/batch)\n",
      "Global Step:    880, accuracy:  25.8%, loss = 1.96 (932.6 examples/sec, 0.27 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step:    890, accuracy:  27.7%, loss = 1.89 (883.9 examples/sec, 0.29 sec/batch)\n",
      "Global Step:    900, accuracy:  27.0%, loss = 1.96 (1039.9 examples/sec, 0.25 sec/batch)\n",
      "Accuracy on Test-Set: 26.52% (2652 / 10000)\n",
      "Saved checkpoint.\n",
      "Global Step:    910, accuracy:  31.6%, loss = 1.92 (968.9 examples/sec, 0.26 sec/batch)\n",
      "Global Step:    920, accuracy:  25.4%, loss = 2.00 (942.1 examples/sec, 0.27 sec/batch)\n",
      "Global Step:    930, accuracy:  36.3%, loss = 1.85 (1034.5 examples/sec, 0.25 sec/batch)\n",
      "Global Step:    940, accuracy:  28.5%, loss = 1.98 (935.7 examples/sec, 0.27 sec/batch)\n",
      "Global Step:    950, accuracy:  29.3%, loss = 1.95 (938.2 examples/sec, 0.27 sec/batch)\n",
      "Global Step:    960, accuracy:  28.5%, loss = 1.96 (940.8 examples/sec, 0.27 sec/batch)\n",
      "Global Step:    970, accuracy:  25.8%, loss = 1.95 (911.2 examples/sec, 0.28 sec/batch)\n",
      "Global Step:    980, accuracy:  31.2%, loss = 1.89 (942.7 examples/sec, 0.27 sec/batch)\n",
      "Global Step:    990, accuracy:  27.0%, loss = 1.98 (937.6 examples/sec, 0.27 sec/batch)\n",
      "Global Step:   1000, accuracy:  24.6%, loss = 1.98 (886.3 examples/sec, 0.29 sec/batch)\n",
      "Accuracy on Test-Set: 28.25% (2825 / 10000)\n",
      "Saved checkpoint.\n",
      "Global Step:   1010, accuracy:  21.5%, loss = 1.96 (963.8 examples/sec, 0.27 sec/batch)\n",
      "Global Step:   1020, accuracy:  32.0%, loss = 1.91 (909.7 examples/sec, 0.28 sec/batch)\n",
      "Global Step:   1030, accuracy:  29.3%, loss = 1.94 (950.5 examples/sec, 0.27 sec/batch)\n",
      "Global Step:   1040, accuracy:  27.3%, loss = 2.00 (940.9 examples/sec, 0.27 sec/batch)\n",
      "Global Step:   1050, accuracy:  30.9%, loss = 1.96 (945.1 examples/sec, 0.27 sec/batch)\n",
      "Global Step:   1060, accuracy:  25.0%, loss = 1.97 (940.0 examples/sec, 0.27 sec/batch)\n",
      "Global Step:   1070, accuracy:  31.2%, loss = 1.94 (942.4 examples/sec, 0.27 sec/batch)\n",
      "Global Step:   1080, accuracy:  30.1%, loss = 1.91 (921.7 examples/sec, 0.28 sec/batch)\n",
      "Global Step:   1090, accuracy:  32.0%, loss = 1.92 (922.7 examples/sec, 0.28 sec/batch)\n",
      "Global Step:   1100, accuracy:  28.9%, loss = 1.94 (937.8 examples/sec, 0.27 sec/batch)\n",
      "Accuracy on Test-Set: 29.25% (2925 / 10000)\n",
      "Saved checkpoint.\n",
      "Global Step:   1110, accuracy:  31.6%, loss = 1.95 (1039.6 examples/sec, 0.25 sec/batch)\n",
      "Global Step:   1120, accuracy:  26.6%, loss = 2.00 (918.8 examples/sec, 0.28 sec/batch)\n",
      "Global Step:   1130, accuracy:  28.5%, loss = 1.92 (965.3 examples/sec, 0.27 sec/batch)\n",
      "Global Step:   1140, accuracy:  27.7%, loss = 1.91 (1039.8 examples/sec, 0.25 sec/batch)\n",
      "Global Step:   1150, accuracy:  25.4%, loss = 1.89 (975.5 examples/sec, 0.26 sec/batch)\n",
      "Global Step:   1160, accuracy:  29.3%, loss = 1.87 (980.1 examples/sec, 0.26 sec/batch)\n",
      "Global Step:   1170, accuracy:  30.1%, loss = 1.87 (934.2 examples/sec, 0.27 sec/batch)\n",
      "Global Step:   1180, accuracy:  28.1%, loss = 2.02 (1027.9 examples/sec, 0.25 sec/batch)\n",
      "Global Step:   1190, accuracy:  28.9%, loss = 1.87 (853.3 examples/sec, 0.30 sec/batch)\n",
      "Global Step:   1200, accuracy:  27.0%, loss = 1.88 (975.1 examples/sec, 0.26 sec/batch)\n",
      "Accuracy on Test-Set: 29.89% (2989 / 10000)\n",
      "Saved checkpoint.\n",
      "Global Step:   1210, accuracy:  28.9%, loss = 1.89 (910.3 examples/sec, 0.28 sec/batch)\n",
      "Global Step:   1220, accuracy:  27.3%, loss = 1.92 (900.5 examples/sec, 0.28 sec/batch)\n",
      "Global Step:   1230, accuracy:  32.4%, loss = 1.96 (959.7 examples/sec, 0.27 sec/batch)\n",
      "Global Step:   1240, accuracy:  28.5%, loss = 1.96 (907.5 examples/sec, 0.28 sec/batch)\n",
      "Global Step:   1250, accuracy:  28.9%, loss = 1.93 (907.7 examples/sec, 0.28 sec/batch)\n",
      "Global Step:   1260, accuracy:  28.5%, loss = 1.96 (913.4 examples/sec, 0.28 sec/batch)\n",
      "Global Step:   1270, accuracy:  36.7%, loss = 1.82 (888.0 examples/sec, 0.29 sec/batch)\n",
      "Global Step:   1280, accuracy:  30.5%, loss = 1.89 (947.0 examples/sec, 0.27 sec/batch)\n",
      "Global Step:   1290, accuracy:  30.1%, loss = 1.89 (947.6 examples/sec, 0.27 sec/batch)\n",
      "Global Step:   1300, accuracy:  28.5%, loss = 1.90 (936.1 examples/sec, 0.27 sec/batch)\n",
      "Accuracy on Test-Set: 30.10% (3010 / 10000)\n",
      "Saved checkpoint.\n",
      "Global Step:   1310, accuracy:  29.7%, loss = 1.87 (912.6 examples/sec, 0.28 sec/batch)\n",
      "Global Step:   1320, accuracy:  27.3%, loss = 1.90 (1003.8 examples/sec, 0.26 sec/batch)\n",
      "Global Step:   1330, accuracy:  27.0%, loss = 1.90 (1004.2 examples/sec, 0.25 sec/batch)\n",
      "Global Step:   1340, accuracy:  31.6%, loss = 1.83 (906.0 examples/sec, 0.28 sec/batch)\n",
      "Global Step:   1350, accuracy:  33.2%, loss = 1.83 (912.9 examples/sec, 0.28 sec/batch)\n",
      "Global Step:   1360, accuracy:  31.6%, loss = 1.87 (908.7 examples/sec, 0.28 sec/batch)\n",
      "Global Step:   1370, accuracy:  29.7%, loss = 1.90 (916.5 examples/sec, 0.28 sec/batch)\n",
      "Global Step:   1380, accuracy:  32.4%, loss = 1.82 (915.9 examples/sec, 0.28 sec/batch)\n",
      "Global Step:   1390, accuracy:  32.4%, loss = 1.84 (885.7 examples/sec, 0.29 sec/batch)\n",
      "Global Step:   1400, accuracy:  34.8%, loss = 1.77 (915.1 examples/sec, 0.28 sec/batch)\n",
      "Accuracy on Test-Set: 30.93% (3093 / 10000)\n",
      "Saved checkpoint.\n",
      "Global Step:   1410, accuracy:  29.7%, loss = 1.85 (869.9 examples/sec, 0.29 sec/batch)\n",
      "Global Step:   1420, accuracy:  34.8%, loss = 1.86 (895.7 examples/sec, 0.29 sec/batch)\n",
      "Global Step:   1430, accuracy:  34.4%, loss = 1.79 (852.2 examples/sec, 0.30 sec/batch)\n",
      "Global Step:   1440, accuracy:  27.0%, loss = 1.88 (861.5 examples/sec, 0.30 sec/batch)\n",
      "Global Step:   1450, accuracy:  30.5%, loss = 1.87 (855.4 examples/sec, 0.30 sec/batch)\n",
      "Global Step:   1460, accuracy:  32.8%, loss = 1.79 (885.0 examples/sec, 0.29 sec/batch)\n",
      "Global Step:   1470, accuracy:  29.3%, loss = 1.84 (860.6 examples/sec, 0.30 sec/batch)\n",
      "Global Step:   1480, accuracy:  30.1%, loss = 1.93 (846.8 examples/sec, 0.30 sec/batch)\n",
      "Global Step:   1490, accuracy:  30.9%, loss = 1.88 (863.1 examples/sec, 0.30 sec/batch)\n",
      "Global Step:   1500, accuracy:  29.7%, loss = 1.85 (897.1 examples/sec, 0.29 sec/batch)\n",
      "Accuracy on Test-Set: 31.76% (3176 / 10000)\n",
      "Saved checkpoint.\n",
      "Global Step:   1510, accuracy:  32.8%, loss = 1.82 (947.7 examples/sec, 0.27 sec/batch)\n",
      "Global Step:   1520, accuracy:  32.8%, loss = 1.79 (950.3 examples/sec, 0.27 sec/batch)\n",
      "Global Step:   1530, accuracy:  33.6%, loss = 1.86 (1010.9 examples/sec, 0.25 sec/batch)\n",
      "Global Step:   1540, accuracy:  30.5%, loss = 1.82 (948.1 examples/sec, 0.27 sec/batch)\n",
      "Global Step:   1550, accuracy:  24.2%, loss = 1.95 (915.8 examples/sec, 0.28 sec/batch)\n",
      "Global Step:   1560, accuracy:  31.6%, loss = 1.83 (879.4 examples/sec, 0.29 sec/batch)\n",
      "Global Step:   1570, accuracy:  30.5%, loss = 1.78 (861.8 examples/sec, 0.30 sec/batch)\n",
      "Global Step:   1580, accuracy:  28.9%, loss = 1.86 (949.8 examples/sec, 0.27 sec/batch)\n",
      "Global Step:   1590, accuracy:  31.2%, loss = 1.85 (914.9 examples/sec, 0.28 sec/batch)\n",
      "Global Step:   1600, accuracy:  32.4%, loss = 1.86 (940.1 examples/sec, 0.27 sec/batch)\n",
      "Accuracy on Test-Set: 32.63% (3263 / 10000)\n",
      "Saved checkpoint.\n",
      "Global Step:   1610, accuracy:  28.5%, loss = 1.85 (878.9 examples/sec, 0.29 sec/batch)\n",
      "Global Step:   1620, accuracy:  34.0%, loss = 1.77 (920.8 examples/sec, 0.28 sec/batch)\n",
      "Global Step:   1630, accuracy:  30.5%, loss = 1.79 (935.4 examples/sec, 0.27 sec/batch)\n",
      "Global Step:   1640, accuracy:  34.4%, loss = 1.81 (938.1 examples/sec, 0.27 sec/batch)\n",
      "Global Step:   1650, accuracy:  30.9%, loss = 1.87 (936.5 examples/sec, 0.27 sec/batch)\n",
      "Global Step:   1660, accuracy:  34.0%, loss = 1.76 (1035.0 examples/sec, 0.25 sec/batch)\n",
      "Global Step:   1670, accuracy:  32.8%, loss = 1.75 (933.0 examples/sec, 0.27 sec/batch)\n",
      "Global Step:   1680, accuracy:  31.6%, loss = 1.80 (970.5 examples/sec, 0.26 sec/batch)\n",
      "Global Step:   1690, accuracy:  32.0%, loss = 1.81 (940.8 examples/sec, 0.27 sec/batch)\n",
      "Global Step:   1700, accuracy:  32.0%, loss = 1.81 (1020.0 examples/sec, 0.25 sec/batch)\n",
      "Accuracy on Test-Set: 33.55% (3355 / 10000)\n",
      "Saved checkpoint.\n",
      "Global Step:   1710, accuracy:  33.2%, loss = 1.77 (939.0 examples/sec, 0.27 sec/batch)\n",
      "Global Step:   1720, accuracy:  32.4%, loss = 1.85 (935.0 examples/sec, 0.27 sec/batch)\n",
      "Global Step:   1730, accuracy:  36.7%, loss = 1.82 (939.0 examples/sec, 0.27 sec/batch)\n",
      "Global Step:   1740, accuracy:  39.5%, loss = 1.71 (938.7 examples/sec, 0.27 sec/batch)\n",
      "Global Step:   1750, accuracy:  35.5%, loss = 1.81 (884.8 examples/sec, 0.29 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step:   1760, accuracy:  35.9%, loss = 1.74 (934.9 examples/sec, 0.27 sec/batch)\n",
      "Global Step:   1770, accuracy:  32.4%, loss = 1.78 (910.8 examples/sec, 0.28 sec/batch)\n",
      "Global Step:   1780, accuracy:  30.5%, loss = 1.77 (937.7 examples/sec, 0.27 sec/batch)\n",
      "Global Step:   1790, accuracy:  32.8%, loss = 1.77 (1037.7 examples/sec, 0.25 sec/batch)\n",
      "Global Step:   1800, accuracy:  34.0%, loss = 1.78 (946.6 examples/sec, 0.27 sec/batch)\n",
      "Accuracy on Test-Set: 33.99% (3399 / 10000)\n",
      "Saved checkpoint.\n",
      "Global Step:   1810, accuracy:  33.6%, loss = 1.85 (914.3 examples/sec, 0.28 sec/batch)\n",
      "Global Step:   1820, accuracy:  31.6%, loss = 1.82 (960.9 examples/sec, 0.27 sec/batch)\n",
      "Global Step:   1830, accuracy:  31.2%, loss = 1.84 (1037.7 examples/sec, 0.25 sec/batch)\n",
      "Global Step:   1840, accuracy:  28.1%, loss = 1.82 (937.8 examples/sec, 0.27 sec/batch)\n",
      "Global Step:   1850, accuracy:  41.4%, loss = 1.69 (966.4 examples/sec, 0.26 sec/batch)\n",
      "Global Step:   1860, accuracy:  34.0%, loss = 1.83 (1028.2 examples/sec, 0.25 sec/batch)\n",
      "Global Step:   1870, accuracy:  35.9%, loss = 1.76 (861.0 examples/sec, 0.30 sec/batch)\n",
      "Global Step:   1880, accuracy:  33.2%, loss = 1.73 (934.7 examples/sec, 0.27 sec/batch)\n",
      "Global Step:   1890, accuracy:  38.3%, loss = 1.68 (891.1 examples/sec, 0.29 sec/batch)\n",
      "Global Step:   1900, accuracy:  30.9%, loss = 1.80 (1036.2 examples/sec, 0.25 sec/batch)\n",
      "Accuracy on Test-Set: 34.78% (3478 / 10000)\n",
      "Saved checkpoint.\n",
      "Global Step:   1910, accuracy:  33.6%, loss = 1.73 (1037.0 examples/sec, 0.25 sec/batch)\n",
      "Global Step:   1920, accuracy:  37.1%, loss = 1.77 (941.5 examples/sec, 0.27 sec/batch)\n"
     ]
    }
   ],
   "source": [
    "if _ITERATION != 0:\n",
    "    train(_ITERATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = tf.get_default_graph()\n",
    "feed = g.get_tensor_by_name('data/images:0')\n",
    "fetch = g.get_tensor_by_name('output/output:0')\n",
    "\n",
    "# Feeding 3 images through the net just for testing\n",
    "feed_vals = img_train_x[0:3]\n",
    "res = sess.run(fetch, feed_dict={feed:feed_vals})\n",
    "np.shape(feed_vals), res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for embedding\n",
    "N = 2000\n",
    "p = 10\n",
    "EMB = np.zeros((N, p), dtype='float32')\n",
    "for i in range(N): #Of course you could do mini-batches\n",
    "    EMB[i] = sess.run(fetch, feed_dict={feed: img_train_x[i:i+1,:]})\n",
    "    if (i % 500 == 0 or i < 5):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = _SAVE_PATH\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "import os\n",
    "# The embedding variable, which needs to be stored\n",
    "# Note this must a Variable not a Tensor!\n",
    "embedding_var = tf.Variable(EMB,  name='Embedding_of_output')\n",
    "sess.run(embedding_var.initializer)\n",
    "summary_writer = tf.summary.FileWriter(LOG_DIR)\n",
    "config = projector.ProjectorConfig()\n",
    "embedding = config.embeddings.add()\n",
    "embedding.tensor_name = embedding_var.name\n",
    "\n",
    "# Comment out if you don't have metadata\n",
    "embedding.metadata_path = os.path.join(LOG_DIR, 'metadata.tsv')\n",
    "\n",
    "# Comment out if you don't want sprites\n",
    "embedding.sprite.image_path = os.path.join(LOG_DIR, 'sprite.png')\n",
    "embedding.sprite.single_image_dim.extend([img_train_x.shape[1], img_train_x.shape[1]])\n",
    "\n",
    "projector.visualize_embeddings(summary_writer, config)\n",
    "saver = tf.train.Saver([embedding_var])\n",
    "saver.save(sess, os.path.join(LOG_DIR, 'model2.ckpt'), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_file = open(os.path.join(LOG_DIR, 'metadata.tsv'), 'w')\n",
    "metadata_file.write('Name\\tClass\\n')\n",
    "print(train_y[:5])\n",
    "print(train_l)\n",
    "\n",
    "print(train_l[np.argmax(train_y[0])])\n",
    "print(train_l[np.argmax(train_y[1])])\n",
    "print(train_l[np.argmax(train_y[2])])\n",
    "\n",
    "for i in range(N):\n",
    "    metadata_file.write('%06d\\t%s\\n' % (i, train_l[np.argmax(train_y[i])]))\n",
    "metadata_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Taken from: https://github.com/tensorflow/tensorflow/issues/6322\n",
    "def images_to_sprite(data):\n",
    "    \"\"\"Creates the sprite image along with any necessary padding\n",
    "\n",
    "    Args:\n",
    "      data: NxHxW[x3] tensor containing the images.\n",
    "\n",
    "    Returns:\n",
    "      data: Properly shaped HxWx3 image with any necessary padding.\n",
    "    \"\"\"\n",
    "    if len(data.shape) == 3:\n",
    "        data = np.tile(data[...,np.newaxis], (1,1,1,3))\n",
    "    data = data.astype(np.float32)\n",
    "    min = np.min(data.reshape((data.shape[0], -1)), axis=1)\n",
    "    data = (data.transpose(1,2,3,0) - min).transpose(3,0,1,2)\n",
    "    max = np.max(data.reshape((data.shape[0], -1)), axis=1)\n",
    "    data = (data.transpose(1,2,3,0) / max).transpose(3,0,1,2)\n",
    "    # Inverting the colors seems to look better for MNIST\n",
    "    #data = 1 - data\n",
    "\n",
    "    n = int(np.ceil(np.sqrt(data.shape[0])))\n",
    "    padding = ((0, n ** 2 - data.shape[0]), (0, 0),\n",
    "            (0, 0)) + ((0, 0),) * (data.ndim - 3)\n",
    "    data = np.pad(data, padding, mode='constant',\n",
    "            constant_values=0)\n",
    "    # Tile the individual thumbnails into an image.\n",
    "    data = data.reshape((n, n) + data.shape[1:]).transpose((0, 2, 1, 3)\n",
    "            + tuple(range(4, data.ndim + 1)))\n",
    "    data = data.reshape((n * data.shape[1], n * data.shape[3]) + data.shape[4:])\n",
    "    data = (data * 255).astype(np.uint8)\n",
    "    return data\n",
    "\n",
    "sprite = images_to_sprite(img_train_x[:N])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "scipy.misc.imsave(os.path.join(LOG_DIR, 'sprite.png'), sprite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls -lhl /tmp/tensorboard/cifar-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
