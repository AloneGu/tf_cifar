{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from time import time\n",
    "\n",
    "from data_util import get_data_set, maybe_download_and_extract\n",
    "from model_util import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name Fully connected layers/fc1 is illegal; using Fully_connected_layers/fc1 instead.\n",
      "INFO:tensorflow:Summary name Fully connected layers/fc1 is illegal; using Fully_connected_layers/fc1 instead.\n",
      "INFO:tensorflow:Summary name Fully connected layers/fc2 is illegal; using Fully_connected_layers/fc2 instead.\n",
      "INFO:tensorflow:Summary name Fully connected layers/fc2 is illegal; using Fully_connected_layers/fc2 instead.\n",
      "INFO:tensorflow:Summary name Fully connected layers/output is illegal; using Fully_connected_layers/output instead.\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, train_l = get_data_set(cifar=10)\n",
    "test_x, test_y, test_l = get_data_set(\"test\", cifar=10)\n",
    "\n",
    "x, y, output, global_step, y_pred_cls = model()\n",
    "\n",
    "_IMG_SIZE = 32\n",
    "_NUM_CHANNELS = 3\n",
    "_BATCH_SIZE = 128\n",
    "_CLASS_SIZE = 10\n",
    "_ITERATION = 200\n",
    "_SAVE_PATH = \"/tmp/tensorboard/cifar-10/\"\n",
    "\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=y))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=1e-4).minimize(loss, global_step=global_step)\n",
    "\n",
    "\n",
    "correct_prediction = tf.equal(y_pred_cls, tf.argmax(y, dimension=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "tf.summary.scalar(\"Accuracy/train\", accuracy)\n",
    "\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "train_writer = tf.summary.FileWriter(_SAVE_PATH, sess.graph)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3072)\n",
      "(50000, 10)\n",
      "['airplane', 'automobile', 'bird', 'cat', 'deer']\n",
      "(50000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(train_l[:5])\n",
    "img_train_x = train_x.reshape(-1,32,32,3)\n",
    "print(img_train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(num_iterations):\n",
    "    '''\n",
    "        Train CNN\n",
    "    '''\n",
    "    for i in range(num_iterations):\n",
    "        randidx = np.random.randint(len(train_x), size=_BATCH_SIZE)\n",
    "        batch_xs = train_x[randidx]\n",
    "        batch_ys = train_y[randidx]\n",
    "\n",
    "        start_time = time()\n",
    "        i_global, _ = sess.run([global_step, optimizer], feed_dict={x: batch_xs, y: batch_ys})\n",
    "        duration = time() - start_time\n",
    "\n",
    "        if (i_global % 10 == 0) or (i == num_iterations - 1):\n",
    "            _loss, batch_acc = sess.run([loss, accuracy], feed_dict={x: batch_xs, y: batch_ys})\n",
    "            msg = \"Global Step: {0:>6}, accuracy: {1:>6.1%}, loss = {2:.2f} ({3:.1f} examples/sec, {4:.2f} sec/batch)\"\n",
    "            print(msg.format(i_global, batch_acc, _loss, _BATCH_SIZE / duration, duration))\n",
    "\n",
    "        if (i_global % 100 == 0) or (i == num_iterations - 1):\n",
    "            \n",
    "            \n",
    "            data_merged, global_1 = sess.run([merged, global_step], feed_dict={x: batch_xs, y: batch_ys})\n",
    "            acc = predict_test()\n",
    "\n",
    "            summary = tf.Summary(value=[\n",
    "                tf.Summary.Value(tag=\"Accuracy/test\", simple_value=acc),\n",
    "            ])\n",
    "            train_writer.add_summary(data_merged, global_1)\n",
    "            train_writer.add_summary(summary, global_1)\n",
    "\n",
    "            saver.save(sess, save_path=_SAVE_PATH, global_step=global_step)\n",
    "            print(\"Saved checkpoint.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_test(show_confusion_matrix=False):\n",
    "    '''\n",
    "        Make prediction for all images in test_x\n",
    "    '''\n",
    "    i = 0\n",
    "    predicted_class = np.zeros(shape=len(test_x), dtype=np.int)\n",
    "    while i < len(test_x):\n",
    "        j = min(i + _BATCH_SIZE, len(test_x))\n",
    "        batch_xs = test_x[i:j, :]\n",
    "        batch_ys = test_y[i:j, :]\n",
    "        predicted_class[i:j] = sess.run(y_pred_cls, feed_dict={x: batch_xs, y: batch_ys})\n",
    "        i = j\n",
    "\n",
    "    correct = (np.argmax(test_y, axis=1) == predicted_class)\n",
    "    acc = correct.mean()*100\n",
    "    correct_numbers = correct.sum()\n",
    "    print(\"Accuracy on Test-Set: {0:.2f}% ({1} / {2})\".format(acc, correct_numbers, len(test_x)))\n",
    "\n",
    "    if show_confusion_matrix is True:\n",
    "        cm = confusion_matrix(y_true=np.argmax(test_y, axis=1), y_pred=predicted_class)\n",
    "        for i in range(_CLASS_SIZE):\n",
    "            class_name = \"({}) {}\".format(i, test_l[i])\n",
    "            print(cm[i, :], class_name)\n",
    "        class_numbers = [\" ({0})\".format(i) for i in range(_CLASS_SIZE)]\n",
    "        print(\"\".join(class_numbers))\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to restore last checkpoint ...\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tensorboard/cifar-10/model2.ckpt-1\n",
      "Failed to restore checkpoint. Initializing variables instead.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(\"Trying to restore last checkpoint ...\")\n",
    "    last_chk_path = tf.train.latest_checkpoint(checkpoint_dir=_SAVE_PATH)\n",
    "    saver.restore(sess, save_path=last_chk_path)\n",
    "    print(\"Restored checkpoint from:\", last_chk_path)\n",
    "except:\n",
    "    print(\"Failed to restore checkpoint. Initializing variables instead.\")\n",
    "    sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step:     10, accuracy:  10.2%, loss = 2.30 (42.1 examples/sec, 3.04 sec/batch)\n"
     ]
    }
   ],
   "source": [
    "if _ITERATION != 0:\n",
    "    train(_ITERATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = tf.get_default_graph()\n",
    "feed = g.get_tensor_by_name('data/images:0')\n",
    "fetch = g.get_tensor_by_name('output/output:0')\n",
    "\n",
    "# Feeding 3 images through the net just for testing\n",
    "feed_vals = img_train_x[0:3]\n",
    "res = sess.run(fetch, feed_dict={feed:feed_vals})\n",
    "np.shape(feed_vals), res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for embedding\n",
    "N = 1000\n",
    "p = 10\n",
    "EMB = np.zeros((N, p), dtype='float32')\n",
    "for i in range(N): #Of course you could do mini-batches\n",
    "    EMB[i] = sess.run(fetch, feed_dict={feed: img_train_x[i:i+1,:]})\n",
    "    if (i % 500 == 0 or i < 5):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = _SAVE_PATH\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "import os\n",
    "# The embedding variable, which needs to be stored\n",
    "# Note this must a Variable not a Tensor!\n",
    "embedding_var = tf.Variable(EMB,  name='Embedding_of_output')\n",
    "sess.run(embedding_var.initializer)\n",
    "summary_writer = tf.summary.FileWriter(LOG_DIR)\n",
    "config = projector.ProjectorConfig()\n",
    "embedding = config.embeddings.add()\n",
    "embedding.tensor_name = embedding_var.name\n",
    "\n",
    "# Comment out if you don't have metadata\n",
    "embedding.metadata_path = os.path.join(LOG_DIR, 'metadata.tsv')\n",
    "\n",
    "projector.visualize_embeddings(summary_writer, config)\n",
    "saver = tf.train.Saver([embedding_var])\n",
    "saver.save(sess, os.path.join(LOG_DIR, 'model2.ckpt'), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_file = open(os.path.join(LOG_DIR, 'metadata.tsv'), 'w')\n",
    "metadata_file.write('Name\\tClass\\n')\n",
    "print(train_y[:5])\n",
    "print(train_l)\n",
    "\n",
    "print(train_l[np.argmax(train_y[0])])\n",
    "print(train_l[np.argmax(train_y[1])])\n",
    "print(train_l[np.argmax(train_y[2])])\n",
    "\n",
    "for i in range(N):\n",
    "    metadata_file.write('%06d\\t%s\\n' % (i, train_l[np.argmax(train_y[i])]))\n",
    "metadata_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Taken from: https://github.com/tensorflow/tensorflow/issues/6322\n",
    "def images_to_sprite(data):\n",
    "    \"\"\"Creates the sprite image along with any necessary padding\n",
    "\n",
    "    Args:\n",
    "      data: NxHxW[x3] tensor containing the images.\n",
    "\n",
    "    Returns:\n",
    "      data: Properly shaped HxWx3 image with any necessary padding.\n",
    "    \"\"\"\n",
    "    if len(data.shape) == 3:\n",
    "        data = np.tile(data[...,np.newaxis], (1,1,1,3))\n",
    "    data = data.astype(np.float32)\n",
    "    min = np.min(data.reshape((data.shape[0], -1)), axis=1)\n",
    "    data = (data.transpose(1,2,3,0) - min).transpose(3,0,1,2)\n",
    "    max = np.max(data.reshape((data.shape[0], -1)), axis=1)\n",
    "    data = (data.transpose(1,2,3,0) / max).transpose(3,0,1,2)\n",
    "    # Inverting the colors seems to look better for MNIST\n",
    "    #data = 1 - data\n",
    "\n",
    "    n = int(np.ceil(np.sqrt(data.shape[0])))\n",
    "    padding = ((0, n ** 2 - data.shape[0]), (0, 0),\n",
    "            (0, 0)) + ((0, 0),) * (data.ndim - 3)\n",
    "    data = np.pad(data, padding, mode='constant',\n",
    "            constant_values=0)\n",
    "    # Tile the individual thumbnails into an image.\n",
    "    data = data.reshape((n, n) + data.shape[1:]).transpose((0, 2, 1, 3)\n",
    "            + tuple(range(4, data.ndim + 1)))\n",
    "    data = data.reshape((n * data.shape[1], n * data.shape[3]) + data.shape[4:])\n",
    "    data = (data * 255).astype(np.uint8)\n",
    "    return data\n",
    "\n",
    "sprite = images_to_sprite(img_train_x[:N])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "scipy.misc.imsave(os.path.join(LOG_DIR, 'sprite.png'), sprite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls -lhl /tmp/tensorboard/cifar-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
